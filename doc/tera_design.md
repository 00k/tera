# Tera系统设计

## 设计背景
百度的链接处理系统每天处理万亿级的超链数据，在过去，这是一系列Mapreduce的批量过程，对时效性收录很不友好。在新一代搜索引擎架构设计中，我们采用流式、增量处理替代了之前的批量、全量处理。链接从被发现到存入链接库再到被调度程序选出，变成一个全实时的过程。在这个实时处理过程中，无论链接的入库还是选取，都需要对链接库进行修改，存储系统需要支持千万甚至上亿QPS的随机读写。旧的存储系统无论在单机性能，还是在扩展性方面，都无法满足，所以我们设计了Tera。

## 链接存储的需求
1. 数据按序存储
支持主域、站点和前缀等多维度统计、读取。
2. 自动负载均衡  
分区可以动态调整，在一个分区数据量变大或者访问频率过高时，可以自动切分，小的分区也可以自动合并。
3. 记录包含时间戳和多个版本  
对于链接历史抓取状态等记录，需要保留多个版本，对于策略回灌等场景，需要根据时间戳判断，避免旧的数据覆盖新的。
4. 强一致性  
写入成功，立即可被所有的用户看到。
5. 按列存储  
在用户只访问固定的少数几列时，能使用更小的IO，提供更高的性能（相对于访问全记录），要求底层存储将这部分列在物理上单独存储。

## 设计目标
### 实现的功能
1. 全局有序  
key可以是任意字符串(二进制串)，不限长度，比较逻辑可由用户定义，按Key的范围分区，分区内由单机维护有序，分区间由Master维护有序。
2. 自动分片  
用户不需要关心分片信息，系统自动处理热点区间的分裂，数据稀疏区间的合并。
单个分区的数据量超过阈值，自动切分为多个，单个分区的读写频率增高时，自动切分。
3. 自动负载均衡和扩容  
单机上保存多个分区，分区的总大小和总访问量达到阈值时，可以触发将部分分区迁移到空闲的机器。
4. 多版本  
每个字段（单元格）都可以保留指定多个版本，系统自动回收过期版本，用户可以按时间戳存取。
5. 局部性群组(LocalityGroup)  
每个局部性群组内部包含多个列族，列族内列个数不限制，不同局部性群组在物理上单独存储，，以提高访问效率。
6. 行级原子性  
对一行的多列进行一次写入，要么全部成功，要么全部失败。
